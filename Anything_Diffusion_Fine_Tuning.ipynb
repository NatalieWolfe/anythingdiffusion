{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/un1tz3r0/anythingdiffusion/blob/main/Anything_Diffusion_Fine_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hBAjQO1kiEW"
      },
      "source": [
        "![visitors](https://visitor-badge.glitch.me/badge?page_id=fine_tuning_your_own_diffusion_model_using_clip_retrieval_ipynb)\n",
        "\n",
        "# [AnythingDiffusion](https://github.com/un1tz3r0/anythingdiffusion/)\n",
        "#### by [un1tz3r0](https://linktr.ee/un1tz3r0), based on a notebook by [Alex Spirin](https://twitter.com/devdef).\n",
        "\n",
        "A simple colab to fine-tune your very own diffusion models on images from CLIP-retrieval which are nearby a text prompt, and automatically resume training from the last checkpoint.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZ8BNzApp_Xk"
      },
      "source": [
        "# Configure\n",
        "\n",
        "Needs 16gb GPU RAM\n",
        "\n",
        "Works in colab pro and on kaggle "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRp0TCVGG4X1",
        "outputId": "e531d389-fcb6-4fb1-b1e2-a26f9c4c5f75"
      },
      "outputs": [],
      "source": [
        "#@title Check GPU Status\n",
        "import subprocess\n",
        "simple_nvidia_smi_display = False#@param {type:\"boolean\"}\n",
        "if simple_nvidia_smi_display:\n",
        "    #!nvidia-smi\n",
        "    nvidiasmi_output = subprocess.run(['nvidia-smi', '-L'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(nvidiasmi_output)\n",
        "else:\n",
        "    #!nvidia-smi -i 0 -e 0\n",
        "    nvidiasmi_output = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(nvidiasmi_output)\n",
        "    nvidiasmi_ecc_note = subprocess.run(['nvidia-smi', '-i', '0', '-e', '0'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(nvidiasmi_ecc_note)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "M-QtGUPcFsdO"
      },
      "outputs": [],
      "source": [
        "#@markdown This is the name of the subdirectory where your custom model snapshots and logs will be dumped during the training:\n",
        "\n",
        "custom_model_name = \"spiraldiffusion\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Everything, including the dataset and the models and model progress and other training output will be on your drive in <tt>Disco_Diffusion/Fine_Tuning/<error>custom_model_name</error>/</tt>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihEN_8wKEXWG"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtMv2MEzSzjN",
        "outputId": "3098650c-e536-4044-8abe-81882f3bda13"
      },
      "outputs": [],
      "source": [
        "#@markdown Connect with google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-fL3fb8wpxZ",
        "outputId": "8e74b1cd-8ab4-4253-877c-575732866ef5"
      },
      "outputs": [],
      "source": [
        "#@markdown Download and install guided diffusion\n",
        "\n",
        "%cd /content\n",
        "!git clone https://github.com/Sxela/guided-diffusion-sxela\n",
        "%cd /content/guided-diffusion-sxela\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OMuXyRFen5l",
        "outputId": "c3321c8c-78b0-4249-fc95-cbf6468065c9"
      },
      "outputs": [],
      "source": [
        "#@markdown Define some helpers and create directories\n",
        "\n",
        "import pathlib, subprocess, os, sys, ipykernel\n",
        "\n",
        "try:\n",
        "  import google.colab\n",
        "  is_colab = True\n",
        "except:\n",
        "  is_colab = False\n",
        "\n",
        "def createPath(filepath):\n",
        "    os.makedirs(filepath, exist_ok=True)\n",
        "\n",
        "def createParent(filepath):\n",
        "    os.makedirs(os.path.dirname(os.path.abspath(filepath)), exist_ok=True)\n",
        "\n",
        "def pipi(*modulestrs):\n",
        "    res = subprocess.run(['pip', 'install', *modulestrs], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "def wget(url, outputdir=None, filename=None):\n",
        "    if outputdir != None:\n",
        "      res = subprocess.run(['wget', url, '-P', f'{outputdir}'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    elif filename != None:\n",
        "      res = subprocess.run(['wget', url, '-O', f'{filename}'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    else:\n",
        "      res = subprocess.run(['wget', url], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
        "    print(res)\n",
        "\n",
        "google_drive = True\n",
        "\n",
        "\n",
        "if is_colab:\n",
        "    if google_drive is True:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        rootPath = '/content/drive/MyDrive/Disco_Diffusion'\n",
        "    else:\n",
        "        rootPath = '/content'\n",
        "else:\n",
        "    rootPath = os.getcwd()\n",
        "\n",
        "def createPath(filepath):\n",
        "    os.makedirs(filepath, exist_ok=True)\n",
        "\n",
        "def createParent(filepath):\n",
        "    os.makedirs(os.path.dirname(os.path.abspath(filepath)), exist_ok=True)\n",
        "\n",
        "# set up some folders based on the custom_model_name in the form for this cell...\n",
        "\n",
        "finetuningRoot = f\"{rootPath}/Fine_Tuning/{custom_model_name}\"\n",
        "createPath(f\"{finetuningRoot}\")\n",
        "\n",
        "datasetRoot = f\"{finetuningRoot}/dataset\"\n",
        "createPath(f\"{datasetRoot}\")\n",
        "\n",
        "trainingRoot = f\"{finetuningRoot}/training\"\n",
        "createPath(f\"{trainingRoot}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7kNS0XQBIY1"
      },
      "source": [
        "# Get images to train on using CLIP-retrieval\n",
        "\n",
        "Generate a dataset from images retrieved by proximity to a text prompt in the CLIP latent space.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmGjWRMM9_VY",
        "outputId": "81b02d54-8bbc-4c81-efd9-ed863a15cd67"
      },
      "outputs": [],
      "source": [
        "# create the dataset using clip retrieval and aiohttp/aiomultiprocessing to download in paralell\n",
        "import shlex\n",
        "\n",
        "dataset_text_prompt = \"spiral\" #@param {type: \"string\"}\n",
        "dataset_fetch_size = 5000 #@param {type: \"integer\"}\n",
        "dataset_crop_count = 6000 #@param {type: \"integer\"}\n",
        "force_existing_dataset = False #@param {type: \"boolean\"}\n",
        "\n",
        "#@markdown Check `force_existing_dataset` to ignore an existing folder from a previous run and redownload and process the CLIP-retrieval results anyway. If this is not checked, then after the first time this cell runs, it will skip this lengthy process and use the dataset from the previous run.\n",
        "\n",
        "datasetPath = f'{datasetRoot}'\n",
        "if pathlib.Path(datasetPath).exists():\n",
        "  print(f\"Dataset directory: {datasetPath}\\nDataset already exists, skipping clip-retrieval...\")\n",
        "else:\n",
        "  createPath(datasetPath)\n",
        "\n",
        "if (not pathlib.Path(datasetPath+\"/crop\").exists()) or force_existing_dataset: # or len(list(pathlib.Path(datasetPath).iterdir())) < 3':\n",
        "  print(f\"Creating new dataset from clip-retrieval for the prompt: '{dataset_text_prompt}'\")\n",
        "  try:\n",
        "    datasetOutPath=datasetPath+\"/out\"\n",
        "    createPath(datasetOutPath)\n",
        "    createPath(datasetPath+\"/crop\")\n",
        "\n",
        "    pipi(\"click\", \"clip-retrieval\", \"img2dataset\", \"aiomultiprocess\", \"aiohttp\", \"aiofile\")\n",
        "    wget(\"https://gist.githubusercontent.com/un1tz3r0/a18ba5cf48228ca5cabc58d1d556ad0b/raw/76f36ad320b04e6529d6e68f0cfedae7e8542841/clipfetch.py\", filename=\"clipfetch.py\")\n",
        "    wget(\"https://gist.githubusercontent.com/un1tz3r0/bbada1caa66f0e639e9aa74baec53686/raw/dfce021bec5a869551fe5c4912ad7385617a730e/randomcrops.py\", filename=\"randomcrops.py\")\n",
        "\n",
        "    dataset_text_prompt_q = shlex.quote(dataset_text_prompt)\n",
        "    datasetOutPath_q = shlex.quote(datasetOutPath)\n",
        "    !python3 clipfetch.py $dataset_text_prompt_q $datasetOutPath_q --count $dataset_fetch_size --timeout 5 --paralell 25\n",
        "\n",
        "    print(f\"Generating {dataset_crop_count} cropped squares from source images in {datasetOutPath}...\")\n",
        "\n",
        "    import sys\n",
        "    sys.path.append(pathlib.Path(\"./\").absolute())\n",
        "    import randomcrops\n",
        "    randomcrops.randomcrops(datasetPath+\"/out\", datasetPath+\"/crop\", dataset_crop_count, 256, weighting=0.0, withclasses=False, statusinterval=50)\n",
        "\n",
        "    datasetPath = datasetRoot+\"/crop\"\n",
        "    print(f\"Done creating dataset from clip-retrieval in: {datasetPath}\")\n",
        "    #touch(datasetPath+\"/.completed\")\n",
        "  except Exception as err:\n",
        "    import traceback as tb\n",
        "    tb.print_exc(err)\n",
        "    raise err\n",
        "else:\n",
        "  datasetPath = datasetRoot+\"/crop\"\n",
        "  print(f\"Skipped creating dataset from clip-retrieval, found existing dataset in: {datasetPath}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV2gIxZhw2me"
      },
      "source": [
        "# <big><big>Fine Tune</big></big>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqBBkqPjqESf"
      },
      "source": [
        "This will run almost forever, but you should start checking your results at around ~50k iterations. Good results begin to appear at 100-200k iterations, depending on your dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apH5i0hTqz1y",
        "outputId": "e4e31bd7-d03e-4d99-83d1-bd170ca04151"
      },
      "outputs": [],
      "source": [
        "#@markdown # Do the run...\n",
        "\n",
        "import shlex\n",
        "\n",
        "def latest_checkpoint(checkpoint_path, default_model, default_model_url):\n",
        "  import pathlib, os\n",
        "  try:\n",
        "    def kf(f):\n",
        "      return f.lstat().st_mtime\n",
        "    f = str(list(sorted(list(pathlib.Path(checkpoint_path).glob(\"ema_0.9999_*.pt\")), key=kf))[-1])\n",
        "    print(f\"Resuming from latest checkpoint found: {f}\")\n",
        "    return f\n",
        "  except Exception as err:\n",
        "    print(f\"Error finding latest checkpoint in {checkpoint_path}: {err}\")\n",
        "    print(f\"Resuming from default pretrained model: {default_model}\")\n",
        "    if not pathlib.Path(default_model).exists():\n",
        "      print(f\"Downloading default pretrained model from: {default_model_url}\")\n",
        "      wget(default_model_url, filename=default_model)\n",
        "      print(f\"Done!\")\n",
        "    else:\n",
        "      print(f\"Default pretrained model found at: {default_model}\")\n",
        "      print(f\"Skipping model download.\")\n",
        "    return default_model\n",
        "\n",
        "\n",
        "#!wget https://openaipublic.blob.core.windows.net/diffusion/march-2021/lsun_uncond_100M_1200K_bs128.pt\n",
        "MODEL_FLAGS=\"--image_size 256 --num_channels 128 --num_res_blocks 2 --num_heads 1 --learn_sigma True --use_scale_shift_norm False --attention_resolutions 16\"\n",
        "DIFFUSION_FLAGS=\"--diffusion_steps 1000 --noise_schedule linear --rescale_learned_sigmas False --rescale_timesteps False --use_scale_shift_norm False\"\n",
        "RESUME_CHECKPOINT=latest_checkpoint(trainingRoot, f\"{trainingRoot}/lsun_uncond_100M_1200K_bs128.pt\", 'https://openaipublic.blob.core.windows.net/diffusion/march-2021/lsun_uncond_100M_1200K_bs128.pt')\n",
        "TRAIN_FLAGS=f\"--lr 2e-5 --batch_size 4 --save_interval 1000 --log_interval 50  --resume_checkpoint {shlex.quote(RESUME_CHECKPOINT)}\"\n",
        "DATASET_PATH=shlex.quote(datasetPath) #change to point to your dataset path \n",
        "%cd /content/guided-diffusion-sxela\n",
        "!OPENAI_LOGDIR=$trainingRoot python scripts/image_train.py --data_dir $DATASET_PATH $MODEL_FLAGS $DIFFUSION_FLAGS $TRAIN_FLAGS"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "include_colab_link": true,
      "machine_shape": "hm",
      "name": "Anything Diffusion: Fine-Tuning Your Own Diffusion Model Using CLIP-Retrieval.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
